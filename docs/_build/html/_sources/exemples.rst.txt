Exemples
=======

Cette section pr√©sente des exemples d'utilisation de Py Stats Toolkit avec l'architecture polymorphique, les modules avanc√©s et les fonctionnalit√©s d'historique int√©gr√©es.

Fonctionnalit√©s d'Historique
---------------------------

Modules de Base avec Historique
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   import numpy as np
   import pandas as pd
   from py_stats_toolkit.stats.descriptives.basic_stats import BasicStatistics
   from py_stats_toolkit.stats.correlation.correlation import Correlation
   from py_stats_toolkit.stats.regression.regression import Regression

   # Donn√©es de test
   data = pd.DataFrame({
       'x': np.random.normal(0, 1, 100),
       'y': np.random.normal(0, 1, 100),
       'z': np.random.normal(0, 1, 100)
   })

   # Statistiques descriptives avec historique automatique
   stats = BasicStatistics()
   result = stats.process(data)
   print(f"Moyenne: {result['mean']:.4f}")
   
   # Afficher l'historique
   history = stats.get_statistics_history()
   print(f"Total d'analyses: {history['total_analyses']}")
   print(f"Points de donn√©es moyens: {history['average_data_points']:.1f}")

   # Corr√©lation avec historique
   corr = Correlation()
   result = corr.process(data, x_col='x', y_col='y', method='pearson')
   print(f"Coefficient: {result['Coefficient']:.4f}")
   
   # Analyser l'historique des corr√©lations
   corr_history = corr.get_correlation_history()
   print(f"M√©thodes utilis√©es: {corr_history['most_common_methods']}")

   # R√©gression avec historique
   reg = Regression()
   result = reg.process(data, feature_cols=['x', 'y'], target_col='z')
   print(f"R¬≤: {result['R¬≤']:.4f}")
   
   # Analyser l'historique des r√©gressions
   reg_history = reg.get_regression_history()
   print(f"R¬≤ moyen: {reg_history['average_r2']:.4f}")

Modules Utilitaires avec Historique
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   from py_stats_toolkit.utils.data_processor import DataProcessor
   from py_stats_toolkit.utils.data_validator import DataValidator

   # Validation de donn√©es
   validator = DataValidator()
   validation = validator.process(data, validation_type='comprehensive')
   
   print(f"Donn√©es valides: {validation['is_valid']}")
   print(f"Probl√®mes d√©tect√©s: {len(validation['issues'])}")
   print(f"Avertissements: {len(validation['warnings'])}")
   
   # Analyser l'historique des validations
   val_history = validator.get_validation_history()
   print(f"Taux de succ√®s: {val_history['success_rate']:.2%}")

   # Traitement de donn√©es
   processor = DataProcessor()
   
   # Standardisation
   standardized = processor.process(data, operation='standardize')
   print(f"Standardisation: {standardized['operation_info']}")
   
   # Normalisation
   normalized = processor.process(data, operation='normalize')
   print(f"Normalisation: {normalized['operation_info']}")
   
   # Analyser l'historique des traitements
   proc_history = processor.get_processing_history()
   print(f"Op√©rations effectu√©es: {proc_history['most_common_operations']}")

Workflow Complet avec Historique
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   # Workflow complet : Validation ‚Üí Traitement ‚Üí Analyse
   
   # 1. Validation des donn√©es
   validator = DataValidator()
   validation = validator.process(data, validation_type='comprehensive')
   
   if not validation['is_valid']:
       print("Probl√®mes d√©tect√©s:", validation['issues'])
   else:
       print("‚úÖ Donn√©es valid√©es avec succ√®s")
   
   # 2. Traitement des donn√©es
   processor = DataProcessor()
   processed_data = processor.process(data, operation='standardize')
   
   print(f"‚úÖ Donn√©es trait√©es: {processed_data['operation_info']}")
   
   # 3. Analyse statistique
   stats = BasicStatistics()
   result = stats.process(processed_data['processed_data'])
   
   print(f"‚úÖ Analyse termin√©e - Moyenne: {result['mean']:.4f}")
   
   # 4. R√©sum√© de l'historique
   print("\nüìä R√©sum√© de l'historique:")
   print(f"Validations: {validator.get_validation_history()['total_validations']}")
   print(f"Traitements: {processor.get_processing_history()['total_operations']}")
   print(f"Analyses: {stats.get_statistics_history()['total_analyses']}")

Script d'Analyse de la Base
~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   # Utilisation du script d'analyse de la base de donn√©es
   import subprocess
   import sys
   
   # Ex√©cuter le script d'analyse
   result = subprocess.run([sys.executable, 'show_database_summary.py'], 
                         capture_output=True, text=True)
   
   print("R√©sum√© de la base de donn√©es:")
   print(result.stdout)

Architecture Polymorphique
------------------------

Support Multiples Types de Donn√©es
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   import numpy as np
   import pandas as pd
   from py_stats_toolkit import AdvancedStatisticsEngine

   # Donn√©es de test
   data_list = [15, 23, 8, 42, 19, 31, 7, 28, 45, 12]
   data_series = pd.Series(data_list)
   data_array = np.array(data_list)
   data_df = pd.DataFrame({
       'A': data_list,
       'B': [20, 25, 15, 35, 22, 30, 10, 25, 40, 18]
   })

   # M√™me fonction, diff√©rents types d'entr√©e
   engine = AdvancedStatisticsEngine()
   
   # Analyse avec liste Python
   scores_list = engine.get_detailed_scores(data_list)
   print("Scores (liste):", scores_list)
   
   # Analyse avec pandas Series
   scores_series = engine.get_detailed_scores(data_series)
   print("Scores (Series):", scores_series)
   
   # Analyse avec numpy array
   scores_array = engine.get_detailed_scores(data_array)
   print("Scores (array):", scores_array)
   
   # Analyse avec pandas DataFrame
   scores_df = engine.get_detailed_scores(data_df)
   print("Scores (DataFrame):", scores_df)

Factory Pattern et Analyse Automatique
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   from py_stats_toolkit import create_module, analyze_with_all_modules

   # Cr√©ation via factory
   stats_engine = create_module('advanced_statistics')
   anomaly_engine = create_module('anomaly_detection')
   temporal_engine = create_module('temporal_validation')
   scoring_engine = create_module('advanced_scoring')
   
   # Analyse automatique avec tous les modules
   data = [15, 23, 8, 42, 19, 31, 7, 28, 45, 12]
   all_results = analyze_with_all_modules(data)
   
   print("R√©sultats de l'analyse compl√®te:")
   for module_name, results in all_results.items():
       print(f"\n{module_name}:")
       print(f"  Score global: {results.get('global_score', 'N/A')}")
       print(f"  Recommandations: {len(results.get('recommendations', []))}")

Modules Avanc√©s
-------------

Statistiques Avanc√©es
~~~~~~~~~~~~~~~~~~

.. code-block:: python

   import numpy as np
   from py_stats_toolkit import AdvancedStatisticsEngine

   # Donn√©es de test
   data = [15, 23, 8, 42, 19, 31, 7, 28, 45, 12]
   historical_data = [
       [10, 20, 30, 40, 50],
       [5, 15, 25, 35, 45],
       [12, 22, 32, 42, 52]
   ]

   # Cr√©ation du moteur
   engine = AdvancedStatisticsEngine()
   
   # Scores d√©taill√©s
   scores = engine.get_detailed_scores(data)
   print("Scores d√©taill√©s:")
   for score_name, score_value in scores.items():
       print(f"  {score_name}: {score_value:.4f}")
   
   # Test d'√©quiprobabilit√©
   equiprob_test = engine.equiprobability_test(data)
   print(f"\nTest d'√©quiprobabilit√©:")
   print(f"  √âquiprobable: {equiprob_test['is_equiprobable']}")
   print(f"  Confiance: {equiprob_test['confidence']:.4f}")
   print(f"  P-value: {equiprob_test['p_value']:.4f}")
   
   # Score global avec donn√©es historiques
   global_score = engine.global_score(
       data,
       date="2025-01-15",
       historical_data=historical_data
   )
   print(f"\nScore global: {global_score:.4f}")

D√©tection d'Anomalies
~~~~~~~~~~~~~~~~~~

.. code-block:: python

   from py_stats_toolkit import AnomalyDetectionEngine

   # Donn√©es normales et avec anomalies
   normal_data = [
       [10, 20, 30, 40, 50],
       [15, 25, 35, 45, 55],
       [12, 22, 32, 42, 52]
   ]
   
   anomalous_data = [
       [10, 20, 30, 40, 50],
       [15, 25, 35, 45, 55],
       [999, 999, 999, 999, 999],  # Anomalie √©vidente
       [12, 22, 32, 42, 52]
   ]

   # Cr√©ation du moteur
   engine = AnomalyDetectionEngine()
   
   # Analyse des donn√©es normales
   normal_analysis = engine.comprehensive_anomaly_analysis(
       normal_data,
       data_type="generic"
   )
   
   print("Analyse des donn√©es normales:")
   print(f"  Score d'anomalie: {normal_analysis['global_anomaly_score']:.4f}")
   print(f"  √âquiprobable: {normal_analysis['equiprobability_analysis']['is_equiprobable']}")
   
   # Analyse des donn√©es avec anomalies
   anomalous_analysis = engine.comprehensive_anomaly_analysis(
       anomalous_data,
       data_type="generic"
   )
   
   print("\nAnalyse des donn√©es avec anomalies:")
   print(f"  Score d'anomalie: {anomalous_analysis['global_anomaly_score']:.4f}")
   print(f"  Patterns anormaux: {anomalous_analysis['pattern_anomaly_analysis']['has_pattern_anomalies']}")
   print(f"  Recommandations: {anomalous_analysis['recommendations']}")

Validation Temporelle
~~~~~~~~~~~~~~~~~~

.. code-block:: python

   from py_stats_toolkit import TemporalValidationEngine

   # Donn√©es temporelles
   time_series_data = [100, 105, 110, 108, 115, 120, 118, 125, 130, 128]
   dates = [
       "2025-01-01", "2025-01-02", "2025-01-03", "2025-01-04", "2025-01-05",
       "2025-01-06", "2025-01-07", "2025-01-08", "2025-01-09", "2025-01-10"
   ]
   
   # Donn√©es avec tendance
   trend_data = [10, 12, 14, 16, 18, 20, 22, 24, 26, 28]
   
   # Donn√©es avec saisonnalit√©
   seasonal_data = [10, 15, 20, 15, 10, 15, 20, 15, 10, 15]

   # Cr√©ation du moteur
   engine = TemporalValidationEngine()
   
   # Validation temporelle compl√®te
   validation = engine.comprehensive_temporal_validation(
       time_series_data,
       dates=dates
   )
   
   print("Validation temporelle:")
   print(f"  Score temporel global: {validation['global_temporal_score']:.4f}")
   print(f"  Coh√©rence temporelle: {validation['temporal_consistency']['is_consistent']}")
   print(f"  Cycles d√©tect√©s: {validation['cycle_analysis']['has_cycles']}")
   
   # Analyse des tendances
   trend_validation = engine.comprehensive_temporal_validation(
       trend_data,
       dates=dates
   )
   
   print(f"\nAnalyse des tendances:")
   print(f"  Tendance d√©tect√©e: {trend_validation['trend_analysis']['has_trend']}")
   print(f"  Direction: {trend_validation['trend_analysis']['trend_direction']}")
   print(f"  Force: {trend_validation['trend_analysis']['trend_strength']:.4f}")
   
   # Analyse de saisonnalit√©
   seasonal_validation = engine.comprehensive_temporal_validation(
       seasonal_data,
       dates=dates
   )
   
   print(f"\nAnalyse de saisonnalit√©:")
   print(f"  Saisonnalit√© d√©tect√©e: {seasonal_validation['seasonality_analysis']['has_seasonality']}")
   print(f"  Force saisonni√®re: {seasonal_validation['seasonality_analysis']['seasonal_strength']:.4f}")

Scoring Avanc√©
~~~~~~~~~~~~

.. code-block:: python

   from py_stats_toolkit import AdvancedScoringEngine

   # Donn√©es de test
   data = [15, 23, 8, 42, 19, 31, 7, 28, 45, 12]
   reference_data = [20, 25, 15, 35, 22, 30, 10, 25, 40, 18]

   # Cr√©ation du moteur
   engine = AdvancedScoringEngine()
   
   # Scores complets
   scores = engine.get_comprehensive_scores(data)
   print("Scores complets:")
   for score_name, score_value in scores.items():
       print(f"  {score_name}: {score_value:.4f}")
   
   # Scores relatifs
   relative_scores = engine.get_relative_scores(data, reference_data)
   print(f"\nScores relatifs:")
   for score_name, score_value in relative_scores.items():
       print(f"  {score_name}: {score_value:.4f}")
   
   # Scoring pond√©r√©
   weights = {
       'variance': 0.3,
       'coherence': 0.2,
       'fractal': 0.2,
       'entropy': 0.15,
       'lunar': 0.15
   }
   weighted_score = engine.get_weighted_score(
       data,
       weights=weights,
       date="2025-01-15"
   )
   print(f"\nScore pond√©r√©: {weighted_score:.4f}")
   
   # Interpr√©tation des scores
   interpretation = engine.interpret_scores(data)
   print(f"\nInterpr√©tation:")
   print(f"  √âvaluation globale: {interpretation['overall_assessment']}")
   print(f"  Forces: {interpretation['strengths']}")
   print(f"  Faiblesses: {interpretation['weaknesses']}")
   print(f"  Recommandations: {interpretation['recommendations']}")

Modules de Base
-------------

Analyse descriptive
----------------

Moyenne glissante
~~~~~~~~~~~~~~

.. code-block:: python

   import numpy as np
   import pandas as pd
   from py_stats_toolkit import MoyenneGlissanteModule

   # Cr√©ation de donn√©es
   np.random.seed(42)
   data = pd.Series(np.random.normal(0, 1, 1000))

   # Cr√©ation du module
   module = MoyenneGlissanteModule(
       window=20,
       n_jobs=4,
       batch_size=100
   )

   # Calcul de la moyenne glissante
   result = module.process(data)

   # Affichage des r√©sultats
   print(result.head())

   # Visualisation
   import matplotlib.pyplot as plt
   plt.figure(figsize=(12, 6))
   plt.plot(data, label='Donn√©es')
   plt.plot(result, label='Moyenne glissante')
   plt.legend()
   plt.show()

Analyse de corr√©lation
-------------------

Matrice de corr√©lation
~~~~~~~~~~~~~~~~~~

.. code-block:: python

   import numpy as np
   import pandas as pd
   from py_stats_toolkit import CorrelationModule

   # Cr√©ation de donn√©es
   np.random.seed(42)
   data = pd.DataFrame({
       'A': np.random.normal(0, 1, 1000),
       'B': np.random.normal(2, 1.5, 1000),
       'C': np.random.normal(-1, 0.5, 1000)
   })

   # Cr√©ation du module
   module = CorrelationModule(
       method='pearson',
       n_jobs=4
   )

   # Calcul de la matrice de corr√©lation
   result = module.process(data)

   # Affichage des r√©sultats
   print(result)

   # Obtenir les paires de variables corr√©l√©es
   pairs = module.get_correlation_pairs(threshold=0.5)
   print(pairs)

   # Visualisation
   import seaborn as sns
   plt.figure(figsize=(10, 8))
   sns.heatmap(result, annot=True, cmap='coolwarm')
   plt.show()

Analyse probabiliste
-----------------

Ajustement de distribution
~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   import numpy as np
   import pandas as pd
   from py_stats_toolkit import ProbabilistesModule

   # Cr√©ation de donn√©es
   np.random.seed(42)
   data = pd.Series(np.random.normal(0, 1, 1000))

   # Cr√©ation du module
   module = ProbabilistesModule(
       distribution='normal',
       n_jobs=4,
       batch_size=100
   )

   # Ajustement de la distribution
   result = module.process(data)

   # Calcul de la densit√© de probabilit√©
   x = np.linspace(-3, 3, 100)
   pdf = module.probability_density(x)

   # Calcul de la fonction de r√©partition
   cdf = module.cumulative_distribution(x)

   # Visualisation
   fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
   
   # Densit√© de probabilit√©
   ax1.hist(data, bins=30, density=True, alpha=0.6)
   ax1.plot(x, pdf, 'r-', lw=2)
   ax1.set_title('Densit√© de probabilit√©')
   
   # Fonction de r√©partition
   ax2.plot(x, cdf, 'b-', lw=2)
   ax2.set_title('Fonction de r√©partition')
   
   plt.show()

Analyse temporelle
--------------

D√©composition de s√©rie temporelle
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   import numpy as np
   import pandas as pd
   from py_stats_toolkit import TimeSeriesModule

   # Cr√©ation de donn√©es
   np.random.seed(42)
   t = np.arange(1000)
   trend = 0.1 * t
   seasonal = 10 * np.sin(2 * np.pi * t / 100)
   noise = np.random.normal(0, 1, 1000)
   data = pd.Series(trend + seasonal + noise)

   # Cr√©ation du module
   module = TimeSeriesModule(
       period=100,
       n_jobs=4,
       batch_size=100
   )

   # Analyse de la s√©rie temporelle
   result = module.process(data)

   # Obtention des composantes
   trend = module.get_trend()
   seasonal = module.get_seasonality()

   # Visualisation
   fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 12))
   
   # S√©rie originale
   ax1.plot(data)
   ax1.set_title('S√©rie temporelle')
   
   # Tendance
   ax2.plot(trend)
   ax2.set_title('Tendance')
   
   # Saisonnalit√©
   ax3.plot(seasonal)
   ax3.set_title('Saisonnalit√©')
   
   plt.tight_layout()
   plt.show()

Tests statistiques
--------------

Tests de normalit√©
~~~~~~~~~~~~~~

.. code-block:: python

   import numpy as np
   import pandas as pd
   from py_stats_toolkit import TestsModule

   # Cr√©ation de donn√©es
   np.random.seed(42)
   data = pd.Series(np.random.normal(0, 1, 1000))

   # Cr√©ation du module
   module = TestsModule(
       test_type='normality',
       n_jobs=4
   )

   # Test de normalit√©
   result = module.process(data)

   # Affichage des r√©sultats
   print(result)

   # Visualisation
   import scipy.stats as stats
   
   plt.figure(figsize=(10, 5))
   
   # Histogramme
   plt.subplot(121)
   plt.hist(data, bins=30, density=True, alpha=0.6)
   x = np.linspace(-3, 3, 100)
   plt.plot(x, stats.norm.pdf(x, 0, 1), 'r-', lw=2)
   plt.title('Histogramme')
   
   # QQ-plot
   plt.subplot(122)
   stats.probplot(data, dist="norm", plot=plt)
   plt.title('QQ-plot')
   
   plt.tight_layout()
   plt.show()

Visualisation
----------

Graphiques multiples
~~~~~~~~~~~~~~~~

.. code-block:: python

   import numpy as np
   import pandas as pd
   from py_stats_toolkit import VisualisationModule

   # Cr√©ation de donn√©es
   np.random.seed(42)
   data = pd.DataFrame({
       'A': np.random.normal(0, 1, 1000),
       'B': np.random.normal(2, 1.5, 1000),
       'C': np.random.normal(-1, 0.5, 1000)
   })

   # Cr√©ation du module
   viz = VisualisationModule(style='seaborn')

   # Histogramme
   viz.process(data['A'], plot_type='histogram')

   # Matrice de corr√©lation
   viz.process(data, plot_type='correlation')

   # Graphique de distribution
   viz.process(data['A'], plot_type='distribution')

Configuration et Personnalisation
-----------------------------

Configuration des Modules
~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   from py_stats_toolkit import AdvancedStatisticsEngine

   # Cr√©ation et configuration du moteur
   engine = AdvancedStatisticsEngine()
   
   # Configuration personnalis√©e
   engine.configure(
       variance_weight=0.3,
       coherence_weight=0.2,
       fractal_weight=0.2,
       entropy_weight=0.15,
       lunar_weight=0.15,
       custom_threshold=0.05
   )
   
   # R√©cup√©ration des param√®tres
   params = engine.get_parameters()
   print("Param√®tres configur√©s:")
   for key, value in params.items():
       print(f"  {key}: {value}")
   
   # Utilisation avec configuration personnalis√©e
   data = [15, 23, 8, 42, 19, 31, 7, 28, 45, 12]
   scores = engine.get_detailed_scores(data)
   print(f"\nScores avec configuration personnalis√©e:")
   for score_name, score_value in scores.items():
       print(f"  {score_name}: {score_value:.4f}")

H√©ritage et Extension
~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   from py_stats_toolkit import StatisticalModule
   import numpy as np

   class CustomAnalysisModule(StatisticalModule):
       def __init__(self):
           super().__init__()
           self.configure(
               custom_parameter="default_value",
               analysis_type="comprehensive"
           )
       
       def process(self, data):
           # Conversion polymorphique
           if hasattr(data, 'values'):
               data_array = data.values
           elif isinstance(data, list):
               data_array = np.array(data)
           else:
               data_array = data
           
           # Logique personnalis√©e
           mean_val = np.mean(data_array)
           std_val = np.std(data_array)
           
           return {
               "custom_mean": mean_val,
               "custom_std": std_val,
               "custom_score": mean_val / (std_val + 1e-8),
               "analysis_type": self.get_parameters()['analysis_type']
           }
       
       def custom_analysis(self, data):
           """M√©thode personnalis√©e suppl√©mentaire"""
           result = self.process(data)
           result['custom_metric'] = len(data) * result['custom_score']
           return result

   # Utilisation du module personnalis√©
   custom_module = CustomAnalysisModule()
   custom_module.configure(analysis_type="advanced")
   
   data = [15, 23, 8, 42, 19, 31, 7, 28, 45, 12]
   result = custom_module.process(data)
   print("R√©sultats du module personnalis√©:")
   for key, value in result.items():
       print(f"  {key}: {value}")
   
   # Utilisation de la m√©thode personnalis√©e
   custom_result = custom_module.custom_analysis(data)
   print(f"\nM√©trique personnalis√©e: {custom_result['custom_metric']:.4f}")

Voir aussi
--------

* :ref:`guide_utilisation`
* :ref:`api`
* :ref:`installation` 